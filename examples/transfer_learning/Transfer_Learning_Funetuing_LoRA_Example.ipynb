{"cells":[{"cell_type":"markdown","metadata":{"id":"QTbhVXg5VirR"},"source":["# LoRA Demo implementation with PyTorch\n","\n","Let's start by importing the necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztOFCOBEVirS"},"outputs":[],"source":["import torch\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"code","source":["import pdb"],"metadata":{"id":"iRlLcZC6lQ2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMWGnIpQVirS"},"source":["Make the model deterministic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmO1K1nXVirT"},"outputs":[],"source":["# Make torch deterministic\n","_ = torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"reigWof5VirT"},"source":["We will be training a network to classify MNIST digits and then fine-tune the network on a particular digit on which it doesn't perform well."]},{"cell_type":"code","source":["!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJtCtmTS6Tdn","executionInfo":{"status":"ok","timestamp":1726759919675,"user_tz":300,"elapsed":3685,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}},"outputId":"997eef82-4642-41e9-dc44-3483729ae039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-19 15:32:57--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n","--2024-09-19 15:32:58--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-gzip]\n","Saving to: ‘MNIST.tar.gz’\n","\n","MNIST.tar.gz            [          <=>       ]  33.20M  17.0MB/s    in 2.0s    \n","\n","2024-09-19 15:33:00 (17.0 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n","\n","MNIST/\n","MNIST/raw/\n","MNIST/raw/train-labels-idx1-ubyte\n","MNIST/raw/t10k-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-labels-idx1-ubyte\n","MNIST/raw/t10k-images-idx3-ubyte.gz\n","MNIST/raw/train-images-idx3-ubyte\n","MNIST/raw/train-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-images-idx3-ubyte\n","MNIST/raw/train-images-idx3-ubyte.gz\n","MNIST/processed/\n","MNIST/processed/training.pt\n","MNIST/processed/test.pt\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgUgb-EpVirT"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","\n","# Load the MNIST dataset\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","# Create a dataloader for the training\n","train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n","\n","# Load the MNIST test set\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n","\n","# Define the device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"JC_vlWWwVirT"},"source":["Create the Neural Network to classify the digits, making it overly complicated to better show the power of LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weetgNOwVirT"},"outputs":[],"source":["class PretrainedNetwork(nn.Module):\n","    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n","        super(PretrainedNetwork,self).__init__()\n","        self.linear1 = nn.Linear(28*28, hidden_size_1)\n","        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.linear3 = nn.Linear(hidden_size_2, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, img):\n","        x = img.view(-1, 28*28)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        x = self.linear3(x)\n","        return x\n","\n","net = PretrainedNetwork().to(device)"]},{"cell_type":"markdown","metadata":{"id":"ai3UewMkVirU"},"source":["Train the network only for 1 epoch to simulate a complete general pre-training on the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zj0Kl0O8VirU","outputId":"06d0e87a-6082-4654-dc6b-6fc64032569e","executionInfo":{"status":"ok","timestamp":1726760066221,"user_tz":300,"elapsed":75181,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 6000/6000 [01:15<00:00, 79.98it/s, loss=0.237]\n"]}],"source":["def train(train_loader, net, epochs=5, total_iterations_limit=None):\n","    cross_el = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","\n","    total_iterations = 0\n","\n","    for epoch in range(epochs):\n","        net.train()\n","        loss_sum = 0\n","        num_iterations = 0\n","\n","        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n","        if total_iterations_limit is not None:\n","            data_iterator.total = total_iterations_limit\n","        for data in data_iterator:\n","            # pdb.set_trace()\n","            num_iterations += 1\n","            total_iterations += 1\n","            x, y = data\n","            x = x.to(device)\n","            y = y.to(device)\n","            optimizer.zero_grad()\n","            output = net(x.view(-1, 28*28))\n","            loss = cross_el(output, y)\n","            loss_sum += loss.item()\n","            avg_loss = loss_sum / num_iterations\n","            data_iterator.set_postfix(loss=avg_loss)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n","                return\n","\n","train(train_loader, net, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"2ZxxiF0PVirV"},"source":["Keep a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EG3IFk3VirV"},"outputs":[],"source":["original_weights = {}\n","for name, param in net.named_parameters():\n","    original_weights[name] = param.clone().detach()\n"]},{"cell_type":"markdown","metadata":{"id":"ExE2Pb69VirV"},"source":["The performance of the pretrained network.\n","As we can see, the network performs poorly on the digit 9. Let's fine-tune it on the digit 9"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3062,"status":"ok","timestamp":1726760216256,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"},"user_tz":300},"id":"cv8c6-qPVirV","outputId":"bc3e9a70-4e21-43f9-b2fc-dc61a3aaef65"},"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 1000/1000 [00:02<00:00, 339.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.959\n","wrong counts for the digit 0: 5\n","wrong counts for the digit 1: 23\n","wrong counts for the digit 2: 39\n","wrong counts for the digit 3: 50\n","wrong counts for the digit 4: 23\n","wrong counts for the digit 5: 22\n","wrong counts for the digit 6: 49\n","wrong counts for the digit 7: 45\n","wrong counts for the digit 8: 26\n","wrong counts for the digit 9: 127\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["def test():\n","    correct = 0\n","    total = 0\n","\n","    wrong_counts = [0 for i in range(10)]\n","\n","    with torch.no_grad():\n","        for data in tqdm(test_loader, desc='Testing'):\n","            x, y = data\n","            x = x.to(device)\n","            y = y.to(device)\n","            output = net(x.view(-1, 784))\n","            for idx, i in enumerate(output):\n","                if torch.argmax(i) == y[idx]:\n","                    correct +=1\n","                else:\n","                    wrong_counts[y[idx]] +=1\n","                total +=1\n","    print(f'Accuracy: {round(correct/total, 3)}')\n","    for i in range(len(wrong_counts)):\n","        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n","\n","test()"]},{"cell_type":"markdown","metadata":{"id":"R-hxC8SLVirV"},"source":["Let's visualize how many parameters are in the original network, before introducing the LoRA matrices."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1726760227972,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"},"user_tz":300},"id":"DebkG_cKVirV","outputId":"aae8864c-c822-4c05-956f-a57d23f090da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n","Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n","Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n","Total number of parameters: 2,807,010\n"]}],"source":["# Print the size of the weights matrices of the network\n","# Save the count of the total number of parameters\n","total_parameters_original = 0\n","for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n","    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n","    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n","print(f'Total number of parameters: {total_parameters_original:,}')"]},{"cell_type":"markdown","metadata":{"id":"Y3CXIzf3VirV"},"source":["Define the LoRA parameterization as described in the paper.\n","The full detail on how PyTorch parameterizations work is here: https://pytorch.org/tutorials/intermediate/parametrizations.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObhY1HsVVirW"},"outputs":[],"source":["class LoRAParametrization(nn.Module):\n","    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n","        super().__init__()\n","        # Section 4.1 of the paper:\n","        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n","        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n","        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n","        nn.init.normal_(self.lora_A, mean=0, std=1)\n","\n","        # Section 4.1 of the paper:\n","        #   We then scale ∆Wx by α/r , where α is a constant in r.\n","        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n","        #   As a result, we simply set α to the first r we try and do not tune it.\n","        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n","        self.scale = alpha / rank\n","        self.enabled = True\n","\n","    def forward(self, original_weights):\n","        if self.enabled:\n","            # Return W + (B*A)*scale\n","            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n","        else:\n","            return original_weights"]},{"cell_type":"markdown","metadata":{"id":"m2MKC2hjVirW"},"source":["Add the parameterization to our network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBTpQcQWVirW"},"outputs":[],"source":["import torch.nn.utils.parametrize as parametrize\n","\n","def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n","    # Only add the parameterization to the weight matrix, ignore the Bias\n","\n","    # From section 4.2 of the paper:\n","    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n","    #   [...]\n","    #   We leave the empirical investigation of [...], and biases to a future work.\n","\n","    features_in, features_out = layer.weight.shape\n","    return LoRAParametrization(\n","        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n","    )\n","\n","parametrize.register_parametrization(\n","    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",")\n","parametrize.register_parametrization(\n","    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",")\n","parametrize.register_parametrization(\n","    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",")\n","\n","\n","def enable_disable_lora(enabled=True):\n","    for layer in [net.linear1, net.linear2, net.linear3]:\n","        layer.parametrizations[\"weight\"][0].enabled = enabled"]},{"cell_type":"markdown","metadata":{"id":"mCKXCuisVirW"},"source":["Display the number of parameters added by LoRA."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1726761647672,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"},"user_tz":300},"id":"2E-RPjXFVirX","outputId":"6839f5ac-f66c-4de0-8e24-a5cc1872dc0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n","Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n","Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n","Total number of parameters (original): 2,807,010\n","Total number of parameters (original + LoRA): 2,813,804\n","Parameters introduced by LoRA: 6,794\n","Parameters incremment: 0.242%\n"]}],"source":["total_parameters_lora = 0\n","total_parameters_non_lora = 0\n","for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n","    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n","    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n","    print(\n","        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n","    )\n","# The non-LoRA parameters count must match the original network\n","assert total_parameters_non_lora == total_parameters_original\n","print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n","print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n","print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n","parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n","print(f'Parameters incremment: {parameters_incremment:.3f}%')"]},{"cell_type":"code","source":["print(net)\n","for name, param in net.named_parameters():\n","  print(name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5L5TfRaOBA4w","executionInfo":{"status":"ok","timestamp":1726761731645,"user_tz":300,"elapsed":140,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}},"outputId":"c261b976-fe64-4107-ad0b-eb9473f08239"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PretrainedNetwork(\n","  (linear1): ParametrizedLinear(\n","    in_features=784, out_features=1000, bias=True\n","    (parametrizations): ModuleDict(\n","      (weight): ParametrizationList(\n","        (0): LoRAParametrization()\n","      )\n","    )\n","  )\n","  (linear2): ParametrizedLinear(\n","    in_features=1000, out_features=2000, bias=True\n","    (parametrizations): ModuleDict(\n","      (weight): ParametrizationList(\n","        (0): LoRAParametrization()\n","      )\n","    )\n","  )\n","  (linear3): ParametrizedLinear(\n","    in_features=2000, out_features=10, bias=True\n","    (parametrizations): ModuleDict(\n","      (weight): ParametrizationList(\n","        (0): LoRAParametrization()\n","      )\n","    )\n","  )\n","  (relu): ReLU()\n",")\n","linear1.bias\n","linear1.parametrizations.weight.original\n","linear1.parametrizations.weight.0.lora_A\n","linear1.parametrizations.weight.0.lora_B\n","linear2.bias\n","linear2.parametrizations.weight.original\n","linear2.parametrizations.weight.0.lora_A\n","linear2.parametrizations.weight.0.lora_B\n","linear3.bias\n","linear3.parametrizations.weight.original\n","linear3.parametrizations.weight.0.lora_A\n","linear3.parametrizations.weight.0.lora_B\n"]}]},{"cell_type":"markdown","metadata":{"id":"QBFw_7pcVirX"},"source":["Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 9 and only for 100 batches."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5tVCJprVirX","outputId":"ac84efbe-f34c-4fb0-de8f-afeb81d69fcc","executionInfo":{"status":"ok","timestamp":1726761785611,"user_tz":300,"elapsed":1631,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Freezing non-LoRA parameter linear1.bias\n","Freezing non-LoRA parameter linear1.parametrizations.weight.original\n","Freezing non-LoRA parameter linear2.bias\n","Freezing non-LoRA parameter linear2.parametrizations.weight.original\n","Freezing non-LoRA parameter linear3.bias\n","Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1:  99%|█████████▉| 99/100 [00:01<00:00, 75.25it/s, loss=0.0263]\n"]}],"source":["# Freeze the non-Lora parameters\n","for name, param in net.named_parameters():\n","    if 'lora' not in name:\n","        print(f'Freezing non-LoRA parameter {name}')\n","        param.requires_grad = False\n","\n","# Load the MNIST dataset again, by keeping only the digit 9\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","exclude_indices = mnist_trainset.targets == 9\n","mnist_trainset.data = mnist_trainset.data[exclude_indices]\n","mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n","# Create a dataloader for the training\n","train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n","\n","# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n","train(train_loader, net, epochs=1, total_iterations_limit=100)"]},{"cell_type":"markdown","metadata":{"id":"iFx51gfxVirX"},"source":["Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnd-zQ9PVirX"},"outputs":[],"source":["# Check that the frozen parameters are still unchanged by the finetuning\n","assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n","assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n","assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n","\n","enable_disable_lora(enabled=True)\n","# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n","# The original weights have been moved to net.linear1.parametrizations.weight.original\n","# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n","assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n","\n","enable_disable_lora(enabled=False)\n","# If we disable LoRA, the linear1.weight is the original one\n","assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"]},{"cell_type":"markdown","metadata":{"id":"uj5FsYY_VirY"},"source":["Test the network with LoRA enabled (the digit 9 should be classified better)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-S8YxGEVirY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726761846886,"user_tz":300,"elapsed":6790,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}},"outputId":"e7cff2ab-8ced-4dbc-b5f1-2aa0ee165f6d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 1000/1000 [00:06<00:00, 149.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.626\n","wrong counts for the digit 0: 140\n","wrong counts for the digit 1: 714\n","wrong counts for the digit 2: 188\n","wrong counts for the digit 3: 215\n","wrong counts for the digit 4: 341\n","wrong counts for the digit 5: 148\n","wrong counts for the digit 6: 265\n","wrong counts for the digit 7: 778\n","wrong counts for the digit 8: 950\n","wrong counts for the digit 9: 2\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Test with LoRA enabled\n","enable_disable_lora(enabled=True)\n","test()"]},{"cell_type":"markdown","metadata":{"id":"vCAoFQBQVirY"},"source":["Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CADYWfcPVirY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726761871701,"user_tz":300,"elapsed":3190,"user":{"displayName":"Theja Tulabandhula","userId":"09328407495338775591"}},"outputId":"75cfc760-dd23-4b2f-84b2-1bb945a8eea2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 1000/1000 [00:03<00:00, 323.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.959\n","wrong counts for the digit 0: 5\n","wrong counts for the digit 1: 23\n","wrong counts for the digit 2: 39\n","wrong counts for the digit 3: 50\n","wrong counts for the digit 4: 23\n","wrong counts for the digit 5: 22\n","wrong counts for the digit 6: 49\n","wrong counts for the digit 7: 45\n","wrong counts for the digit 8: 26\n","wrong counts for the digit 9: 127\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Test with LoRA disabled\n","enable_disable_lora(enabled=False)\n","test()"]},{"cell_type":"code","source":[],"metadata":{"id":"-_KaJDodBxw5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"https://github.com/hkproj/pytorch-lora/blob/main/lora.ipynb","timestamp":1694118593092}],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}